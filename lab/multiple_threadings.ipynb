{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ba41b9",
   "metadata": {},
   "source": [
    "## Multiple threading python basic\n",
    "\n",
    "FOr I/O bound like reading file, networking (API,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2960ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "def worker(task):\n",
    "    print(f\"{threading.current_thread().name = }\")\n",
    "    print(threading.get_ident())\n",
    "    print(f\"Task {task} running\")\n",
    "\n",
    "# Create a thread pool with 2 workers\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    # Submit two tasks to run in parallel\n",
    "    executor.submit(worker, 1)\n",
    "    executor.submit(worker, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8346a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(f\"{threading.active_count() = }\")\n",
    "print(f\"{threading.enumerate() = }\")\n",
    "\n",
    "def square(num):\n",
    "    print(f\"{threading.current_thread().name = }\")\n",
    "    print(f\"{threading.get_ident() = }\")\n",
    "    print(f\"{threading.get_native_id() = }\")\n",
    "    print(f\"{os.getpid() = }\")\n",
    "    print(f\"{threading.active_count() = }\")\n",
    "    print(f\"{threading.local().__dict__ = }\")\n",
    "    print(f\"Square: {num*num}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "def cube(num):\n",
    "    print(f\"{threading.current_thread().name = }\")\n",
    "    print(f\"{threading.get_ident() = }\")\n",
    "    print(f\"{threading.get_native_id() = }\")\n",
    "    print(f\"{threading.active_count() = }\")\n",
    "    print(f\"{os.getpid() = }\")\n",
    "    print(f\"Cube: {num*num*num}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "t1 = threading.Thread(target=square, args=(4,))\n",
    "t2 = threading.Thread(target=cube, args=(4,))\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "print(f\"{threading.active_count() = }\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc008ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in locals().items():\n",
    "#     print(f\"{_ = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ab877",
   "metadata": {},
   "source": [
    "## Sample, can use for template for crawling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d7e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample, can use for template for crawling data\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def crawl(link, delay=3):\n",
    "    print(f\"crawl started for {link}\")\n",
    "    time.sleep(delay)  # Blocking I/O (simulating a network request)\n",
    "    print(f\"crawl ended for {link}\")\n",
    "\n",
    "links = [\n",
    "    \"https://python.org\",\n",
    "    \"https://docs.python.org\",\n",
    "    \"https://peps.python.org\",\n",
    "]\n",
    "\n",
    "# Start threads for each link\n",
    "threads = []\n",
    "for link in links:\n",
    "    # Using `args` to pass positional arguments and `kwargs` for keyword arguments\n",
    "    t = threading.Thread(target=crawl, args=(link,), kwargs={\"delay\": 2})\n",
    "    threads.append(t)\n",
    "\n",
    "# Start each thread\n",
    "for t in threads:\n",
    "    t.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f317b358",
   "metadata": {},
   "source": [
    "## Can use to check page active or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Can use to check page active or not\n",
    "import concurrent.futures\n",
    "import urllib.request\n",
    "\n",
    "URLS = ['http://www.foxnews.com/',\n",
    "        'http://www.cnn.com/',\n",
    "        'http://europe.wsj.com/',\n",
    "        'http://www.bbc.co.uk/',\n",
    "        'http://nonexistent-subdomain.python.org/']\n",
    "\n",
    "# Retrieve a single page and report the URL and contents\n",
    "def load_url(url, timeout):\n",
    "    with urllib.request.urlopen(url, timeout=timeout) as conn:\n",
    "        return conn.read()\n",
    "\n",
    "# We can use a with statement to ensure threads are cleaned up promptly\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Start the load operations and mark each future with its URL\n",
    "    future_to_url = {executor.submit(load_url, url, 60): url for url in URLS}\n",
    "    print(f\"{future_to_url = }\")\n",
    "    print(f\"{'':=^50}\")\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        print(f\"{future = }\")\n",
    "        url = future_to_url[future]\n",
    "        print(f\"{url = }\")\n",
    "        try:\n",
    "            data = future.result()\n",
    "            # print(f\"{data = }\")\n",
    "        except Exception as exc:\n",
    "            print(f'{url = } generated an exception: {exc = }' )\n",
    "        else:\n",
    "            print(f'{url = } page is {len(data)} bytes')\n",
    "        print(f\"{'':=^50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3480da0b",
   "metadata": {},
   "source": [
    "### Can use multiple thread for reading multiplr files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use multiple thread for reading multiplr files\n",
    "\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "default_workers: int = min(32, os.cpu_count() + 4)\n",
    "\n",
    "FILE_PATHS: list = [\n",
    "    \"/home/user/prj1mrdp/logs/2025-11-15.log\",\n",
    "    \"/home/user/prj1mrdp/logs/2025-11-16.log\",\n",
    "    \"/home/user/prj1mrdp/logs/tracking_error.log\"\n",
    "]\n",
    "\n",
    "def open_file(file_path: str = \"\") -> None:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        try:\n",
    "            data = file.readlines()\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            tb_str: str = traceback.TracebackException.from_exception(e).format()\n",
    "            print(f\"[{open_file.__name__}] Error: {tb_str = }\")\n",
    "            return []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=default_workers) as executor:\n",
    "    read_file = {executor.submit(open_file, file_path): file_path for file_path in FILE_PATHS}\n",
    "    print(f\"{read_file = }\")\n",
    "    for file_readed in concurrent.futures.as_completed(read_file):\n",
    "        print(f\"{file_readed = }\")\n",
    "        file_path = read_file[file_readed]\n",
    "        print(f\"{file_path = }\")\n",
    "        try:\n",
    "            data = file_readed.result()\n",
    "        except Exception as e:\n",
    "            print(f\"{e = }\")\n",
    "        else:\n",
    "            print(f\"{file_path = }, {len(data) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70f503",
   "metadata": {},
   "source": [
    "## How Python decides the default number of workers for threading pool?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Python decides the default number of workers for threading pool?\n",
    "import os\n",
    "print(f\"{os.cpu_count() = }\")\n",
    "default_workers = min(32, os.cpu_count() + 4)\n",
    "default_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a15f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83604e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Source - https://stackoverflow.com/a\n",
    "# # Posted by JimJty, modified by community. See post 'Timeline' for change history\n",
    "# # Retrieved 2025-11-19, License - CC BY-SA 4.0\n",
    "\n",
    "# try:\n",
    "#     # For Python 3\n",
    "#     import queue\n",
    "#     from urllib.request import urlopen\n",
    "# except:\n",
    "#     # For Python 2 \n",
    "#     import Queue as queue\n",
    "#     from urllib2 import urlopen\n",
    "\n",
    "# import threading\n",
    "\n",
    "# worker_data = ['http://google.com', 'http://yahoo.com', 'http://bing.com']\n",
    "\n",
    "# # Load up a queue with your data. This will handle locking\n",
    "# q = queue.Queue()\n",
    "# for url in worker_data:\n",
    "#     q.put(url)\n",
    "\n",
    "# # Define a worker function\n",
    "# def worker(url_queue):\n",
    "#     queue_full = True\n",
    "#     while queue_full:\n",
    "#         try:\n",
    "#             # Get your data off the queue, and do some work\n",
    "#             url = url_queue.get(False)\n",
    "#             data = urlopen(url).read()\n",
    "#             print(len(data))\n",
    "\n",
    "#         except queue.Empty:\n",
    "#             queue_full = False\n",
    "\n",
    "# # Create as many threads as you want\n",
    "# thread_count = 5\n",
    "# for i in range(thread_count):\n",
    "#     t = threading.Thread(target=worker, args = (q,))\n",
    "#     t.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c050049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check url is active or not, using threadpool and requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "\n",
    "urls = ['http://google.com','http://yahoo.com','http://bing.com']\n",
    "\n",
    "def fetch(url, timeout=5):\n",
    "    try:\n",
    "        # Either create a session per thread or use requests.get (stateless)\n",
    "        resp = requests.get(url, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        return url, len(resp.content)\n",
    "    except Exception as e:\n",
    "        return url, e\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as ex:\n",
    "    futures = {ex.submit(fetch, url): url for url in urls}\n",
    "    for fut in as_completed(futures):\n",
    "        url, result = fut.result()\n",
    "        print(url, result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
